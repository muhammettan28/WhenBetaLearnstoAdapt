This repository accompanies the article When Î² Learns to Adapt: Stabilizing Variational Ensembles for Visual Malware Detection and provides minimal, reproducible examples of the five models described in the study.
Each script in the /src directory corresponds to a distinct model configuration, incrementally extending the baseline CNN architecture with feature-fusion, autoencoding, and adaptive Î²-VAE mechanisms.

Repository Structure
â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ model1.py

â”‚   â”œâ”€â”€ model2.py

â”‚   â”œâ”€â”€ model3.py

â”‚   â”œâ”€â”€ model4.py

â”‚   â”œâ”€â”€ model5.py

â”‚
â”œâ”€â”€ data/

â”‚   â””â”€â”€ sample_malimg/        # small subset (e.g., 10 benign, 10 malware images)
â”‚
â””â”€â”€ README.md




âš™ï¸ Model Descriptions

Model 1 â€” Baseline CNNs (Single Backbone Evaluation)

This model trains individual pre-trained convolutional networks (VGG16, VGG19, Xception, ResNet50, ConvNeXtLarge) separately on the visual malware datasets.
It establishes a baseline for per-architecture performance and serves as the foundation for the feature-fusion ensemble.
Each CNN is fine-tuned with frozen ImageNet weights, global average pooling, and fully connected classifier layers.

â¸»

Model 2 â€” Feature Fusion Ensemble

Model 2 integrates the feature representations of multiple CNN backbones through concatenation.
Each backbone acts as a parallel feature extractor, and their flattened outputs are merged to form an extended feature space.
This approach enhances feature diversity and improves multi-scale representation learning across malware families.

â¸»

Model 3 â€” Autoencoder-Enhanced Feature Compression

In this configuration, the fused feature vectors obtained from Model 2 are passed through a symmetric autoencoder block.
The encoder compresses the high-dimensional feature space into latent representations (tested with 32, 128, 256 dimensions), while the decoder reconstructs them to enforce information consistency.
This design evaluates how latent-space compression affects generalization and classification stability.

â¸»

Model 4 â€” Fixed-Î² Variational Autoencoder (Î²-VAE)

Model 4 replaces the deterministic autoencoder with a variational counterpart, introducing a fixed Î² coefficient in the KL-divergence term to control the regularization strength.
Three fixed Î² values (0.05, 0.01, 0.001) are tested across different latent sizes.
While the fixed-Î² approach promotes disentanglement, it often suffers from posterior collapse and unstable loss dynamics as the feature-space dimensionality increases.

â¸»

Model 5 â€” Annealing-Based Î²-VAE (Proposed Model)

Model 5 extends Model 4 with an annealing mechanism that dynamically increases Î² during training, preventing abrupt regularization in early epochs.
This adaptive scheduling stabilizes training, balances reconstruction and regularization terms, and mitigates the exploding-loss problem observed in fixed-Î² configurations.
Additional refinements include:
	â€¢	Label smoothing (Îµ = 0.05) to improve calibration across heterogeneous datasets.
	â€¢	Gradient clipping (clipnorm = 1.0) to control gradient explosion.
	â€¢	ReduceLROnPlateau scheduler for adaptive learning rate decay.
	â€¢	Increased dropout (0.3) for stronger regularization in large feature spaces.

â¸»

ğŸ§© Training Settings
	â€¢	Image resolution: 224 Ã— 224 px
	â€¢	Batch size: 32
	â€¢	Optimizer: Adam (initial learning rate = 1e-4)
	â€¢	Loss: Categorical cross-entropy
	â€¢	Monitoring metric: Validation loss
	â€¢	Early stopping patience: 5 epochs
	â€¢	Epoch limit: 150
